---
title: "Project - Part 2"
author: "Sandy Vo"
date: "7/22/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#load the edited data into the R environment
```{r}
df <- read.csv("C:/Users/tv/Desktop/Machine learning I/dfex.csv")
```

#Factor categorical variables
```{r}
cols <- c("sex","cp","fbs", "restecg", "exang", "slope", "ca", "thal", "target")
df[cols] <- lapply(df[cols], factor)

```

```{r}
df_cor <- data.frame(lapply(df,as.integer))
cor(df_cor)

library(corrplot)
corrplot(cor(df_cor))

```

From the correlation plot, we can see that there is a relatively mild correlation between
predictors such as age and thalach, age and ca, sex and thal, cp and exang, thalach and
exang, slope and oldpeak. So we expect to see lasso regression shrink some less important
coefficients.

Collinearity can be assessed using the R function vif() [car package], which computes
the variance inflation factors.


```{r}
model <- glm(target ~., data = df, 
               family = binomial)
car::vif(model)

```

A VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. In this
dataset, there is no collinearity since all predictors have a value of VIF well below 5.

Checking the linear relationship between numerical predictors and the logit of the
response can be done by visually inspecting the scatter plot between each predictor and the logit values.

```{r}
library(broom)
library(tidyverse)
library(ggplot2)
```

```{r}

probabilities <- predict(model, type = "response")
predicted <- ifelse(probabilities > 0.5, "Yes", "No")
```

```{r}
mydata <- df %>% select_if(is.numeric) %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

```


```{r}
ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

```

The smoothed scatter plots show that variables age, chol, oldpeak, thalach and
trestbps are all quite linearly associated with the diabetes outcome in logit scale. However, the plot between trestbps and logit has a wild tail. It is because there is very few points at the tail.

#Check skewed data

The plot below indicates some insights: age, trestbps and chol are approximately
normally distributed.

oldpeak is left-skewed while it is right-skewed for thalach.

```{r}
library(Hmisc)
continous_features = c('trestbps','age','chol','thalach','oldpeak') 
hist.data.frame(df[,continous_features])
```

However, there is no assumption about normality on independent variable in logistic regression. So the skewness of the numerical predictors is not problematic.

```{r}
ggplot(data = df,aes(x = age, y = thalach, color = target)) + geom_point(alpha = 0.6) + xlab("Age") + ylab("Maximum heart rate") + ggtitle("Heart disease in function of Age and Maximum heart rate") + labs(color = "Heart disease") + theme(plot.title = element_text(hjust = 0.5))
```

A person gets older, their heart rate decreases. We can see a downward trend in the plot
below.

It seems that maximum heart rate can be strong predictor for heart disease, regardless of
age.



